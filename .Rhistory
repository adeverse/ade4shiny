## Plotting variables
CEX <- 1.5
LWD <- 2
## Generate data
n <- 20 # Number of samples
x <- matrix(runif(n, -1, 1), ncol=1) # x between -1 and 1
theta <- 2.3 # slope of linear model
noise.sd <- 0.5 # standard deviation of noise
y <- theta * x + rnorm(length(x), 0, noise.sd) # linear model for y|x.
xgrid <- matrix(seq(-1, 1, by=1e-2), ncol=1) # grid to plot the estimated functions
ygrid <- theta * xgrid + rnorm(length(xgrid), 0, noise.sd) # corresponding ys, only used to estimate the generalization error
## Least square linear regression
ls <- function(x, y){
solve(crossprod(x), crossprod(x, y))
}
## Non-linear map
nl.map <- function(x, degree){
x.pol <- matrix(NA, nrow=length(x), ncol=degree)
x.pol[, 1] <- x
for(jj in 2:degree){
x.pol[, jj] <- x.pol[, jj-1] * x
}
return(x.pol)
}
## Least square polynomial regression
poly <- function(x, y, degree){
## Polynomial is a linear function over a non-linear map of x. Build
## this linear map.
phi.pol <- nl.map(x, degree)
w <- ls(phi.pol, y)
return(list(map=phi.pol, w=w))
}
## Mean square error
rmse <- function(y, yh){
sqrt(mean((y - yh)^2))
}
## Linear estimation
## phi1 <- cbind(rep(1, length(x)), x)
phi1 <- x
w1 <- ls(phi1, y)
## xgrid1 <- cbind(rep(1, length(xgrid)), xgrid)
## Compute linear function
print(sprintf('Linear estimation: theta=%g. True theta=%g', w1, theta))
xgrid1 <- xgrid
yh1 <- xgrid1 %*% w1
## Average error on n observed points
rmse.train.lin <- rmse(y, phi1 %*% w1)
## Average error on a large number of points which were not used to
## learn the function
rmse.test.lin <- rmse(ygrid, xgrid1 %*% w1)
## Polynomial estimation
degree <- 5
## Compute linear function
poly.res <- poly(x, y, degree)
phi.pol <- poly.res$map
w.pol <- poly.res$w
## Print non-linear map of x
## print(head(nl.map(x, degree)))
xgrid.pol <- nl.map(xgrid, degree)
print(sprintf('Polynomial estimation: theta=%s. True theta=%g', paste(sapply(w.pol, round, 2), collapse=' '), theta))
yh.pol <- xgrid.pol %*% w.pol
## Average error on n observed points
rmse.train.pol <- rmse(y, phi.pol %*% w.pol)
## Average error on a large number of points which were not used to
## learn the function
rmse.test.pol <- rmse(ygrid, xgrid.pol %*% w.pol)
## Plot the result
par(mfrow=c(2, 1), mar=c(5,5,4,2))
## Linear function
plot(x, y, xlab='x', ylab='y', main=sprintf('Linear function, training RMSE=%g, test RMSE=%g', rmse.train.lin, rmse.test.lin), pch=19, cex=CEX, cex.lab=CEX)
abline(a=0, b=theta, lty=2) # True function
lines(xgrid, yh1, col='green', lwd=LWD, lty=1)
## Polynomial function
plot(x, y, xlab='x', ylab='y', main=sprintf('Degree %d polynomial, training RMSE=%g, test RMSE=%g', degree, rmse.train.pol, rmse.test.pol), pch=19, cex=CEX, cex.lab=CEX)
abline(a=0, b=theta, lty=2) # True function
lines(xgrid, yh.pol, col='green', lwd=LWD, lty=1)
n.rep <- 1e3
## New n (x, y) samples
w1.rep <- rep(NA, n.rep)
w.pol.rep <- matrix(NA, degree, n.rep)
for(rr in 1:n.rep){
x <- matrix(runif(n, -1, 1), ncol=1)
y <- theta * x + rnorm(length(x), 0, noise.sd)
w1.rep[rr] <- ls(x, y)
w.pol.rep[, rr] <- poly(x, y, degree)$w
}
dev.new()
par(mfrow=c(2, 1), mar=c(5,5,4,2))
## Plot coefficients of linear regression obtained across runs. Dotted
## line indicates the true slope.
boxplot(w1.rep, main=sprintf('%d linear regressions', n.rep))
axis(1, at=1, labels=expression(theta[1]))
abline(h=theta, lty=2)
## Plot polynomial coefficients of each degree obtained across runs. Black dotted
## line indicates the true slope, gray dotted line is 0.
boxplot(t(w.pol.rep), main=sprintf('%d degree %d regressions', n.rep, degree), ylim=c(-20, 20), axes=F)
axis(1, at=1:degree, labels=sapply(1.0*(1:degree), FUN=function(nn) as.expression(bquote(theta[.(nn)]))))
axis(side = 2)
abline(h=theta, lty=2)
abline(h=0, lty=2, col='gray')
## Run linear and polynomial (degree=5) regression. Inspect mapping
## Run linear and polynomial (degree=5) regression. Inspect mapping
## matrix used for polynomial regression. Visually compare how the
## Keep n fixed (n=20) and increase the degree (eg degree=10). What
## -> polynomial oscillates, especially in regions with few points
## Do several runs with n=20 and degree=10. Observe the difference of overfit and variance between the two functions.
## -> Variance: very different polynomials from one run to the other
## -> Variance: very different polynomials from one run to the other
##  (large variation around its mean, the dotted line). The linear
## Keep degree fixed to 10 and increase n. What happens to the
## -> polynomial stops oscillating
## -> polynomial stops oscillating
## -> actually converges to a linear function (and similar RMSEs)
## Run the same experiment 1000 times. Visualize the distribution of
## Run the same experiment 1000 times. Visualize the distribution of
## the regression coefficients. Discuss the difference between theta1
## Run the same experiment 1000 times. Visualize the distribution of
## the regression coefficients. Discuss the difference between theta1
## and the other coefficients. How are the distributions affected by
## -> All estimates are unbiased (! not true for all MLE).
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
## -> All estimates are unbiased (! not true for all MLE).
## -> Larger n leads to lower variance
setwd("C:/Users/marti/Documents/Cours/M2/Projet/shiny_ade4/")
library(shiny)
library(shinydashboard)
library(shinyjs)
library(ade4)
library(adegraphics)
library(DT)
library(xfun)
library(factoextra)
